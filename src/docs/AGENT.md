# üìö Documenta√ß√£o do C√≥digo: Sistema RAG de Informa√ß√µes sobre Autores

## üìù Descri√ß√£o

Este c√≥digo implementa um sistema de Gera√ß√£o Aumentada por Recupera√ß√£o (RAG - Retrieval-Augmented Generation) utilizando o framework LangChain e a API do Google Gemini.

O objetivo principal do sistema √© responder perguntas sobre autores com base em um conjunto de dados fornecido em um arquivo CSV. Ele faz isso processando o CSV, criando uma base de conhecimento vetorial e, em seguida, usando um modelo de linguagem grande (LLM) para formular respostas precisas com base nos trechos de dados recuperados (chunks).

## ‚öôÔ∏è Funcionalidades

* Configura√ß√£o de Logs: Configura um sistema de logging para rastrear o fluxo de execu√ß√£o e erros, salvando logs em agent.log e exibindo-os no console.

* Processamento de CSV: A fun√ß√£o processar_csv_author_preparar_chunks l√™ um arquivo CSV (esperando colunas como author, data nascimento, local nascimento, descricao), transforma cada linha em um Document do LangChain e divide esses documentos em peda√ßos menores (chunks) usando o RecursiveCharacterTextSplitter.

* Cria√ß√£o de Base Vetorial (Vector Store): A fun√ß√£o criar_base_vetorial utiliza embeddings do Google Gemini AI (GoogleGenerativeAIEmbeddings) para converter os chunks de texto em vetores num√©ricos. Esses vetores s√£o armazenados no ChromaDB (em mem√≥ria), criando a base de conhecimento pesquis√°vel.

* Pipeline RAG: Constr√≥i um pipeline completo de RAG usando LangChain Expression Language (LCEL), que inclui:

* Retriever: Busca os 3 (k=3) chunks mais relevantes na base vetorial.

* Prompt Template: Formata a consulta do usu√°rio junto com o contexto recuperado para o LLM.

* LLM: Utiliza o modelo gemini-2.0-flash-001 para gerar a resposta final, seguindo as instru√ß√µes do prompt para ser um especialista em autores.

* Orquestra√ß√£o: O c√≥digo executa sequencialmente o processamento do CSV, a cria√ß√£o da base vetorial e a constru√ß√£o da cadeia RAG, preparando o sistema para receber perguntas.

## üì¶ Depend√™ncias

Este projeto requer as seguintes bibliotecas Python, que devem ser instaladas no ambiente:

* csv (Biblioteca padr√£o do Python)

* logging (Biblioteca padr√£o do Python)

* os (Biblioteca padr√£o do Python)

* getpass (Biblioteca padr√£o do Python)

* langchain-google-genai (Para modelos LLM e Embeddings do Gemini)

* langchain (Framework principal do LangChain)

* langchain-community (Para componentes como Chroma e PyPDFLoader - embora este √∫ltimo n√£o seja usado no fluxo principal)

Nota: A vari√°vel de ambiente GOOGLE_API_KEY √© essencial e √© solicitada ao usu√°rio via getpass se n√£o estiver configurada no ambiente.

## üíª Estrutura do C√≥digo

A estrutura do c√≥digo √© organizada em se√ß√µes l√≥gicas para facilitar a manuten√ß√£o e o entendimento do fluxo de trabalho:

1. Configura√ß√£o Inicial e Importa√ß√µes
    
    * Importa√ß√£o de todas as bibliotecas necess√°rias (csv, logging, componentes do LangChain).

    * Configura√ß√£o do sistema de logging para agent.log e console.

    * Verifica√ß√£o e solicita√ß√£o da Chave API do Google.

```python
import csv
import logging
import os
import getpass

# Modelos e Componentes do LangChain
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.docstore.document import Document
from langchain_community.document_loaders import PyPDFLoader # Importado, mas n√£o usado no fluxo RAG

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('agent.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)    

if not os.getenv('GOOGLE_API_KEY'):
    os.environ['GOOGLE_API_KEY'] = getpass.getpass('Enter your Google API key: ')
```

2. Processamento e Prepara√ß√£o dos Dados CSV (processar_csv_author_preparar_chunks)
    
    * Entrada: Caminho do arquivo CSV.

    * Processo: L√™ o CSV, cria objetos Document combinando as colunas de autor, data, local e descri√ß√£o.

    * Sa√≠da: Lista de chunks de documentos (chunks) prontos para vetoriza√ß√£o.

```python

# --- PROCESSAMENTO E PREPARA√á√ÉO DOS DADOS CSV ---
def processar_csv_author_preparar_chunks(file_path_csv):
    '''
    Converte os dados brutos de arquivos csv em Documentos do LangChain e os divide em chunks.
    '''
    logger.info('Iniciando processamento e chunking dos documentos...')    
    documentos_langchain = []

    with open(file_path_csv, 'r', newline='', encoding='utf-8') as file_csv:
        dados_brutos = csv.DictReader(file_csv)

        for item in dados_brutos:            
            # Combina os dados relevantes em um √∫nico conte√∫do
            conteudo = f'author: {item['author']}\n data nascimento: {item['data nascimento']}\n local nascimento: {item['local nascimento']} \n descricao: {item['descricao']}'
            documento = Document(
                page_content=conteudo,
                metadata={
                    'author': item['author'],
                    'data nascimento': item['data nascimento'],
                    'local nascimento': item['local nascimento'],
                    'descricao': item['descricao'],
                    }
            )
            documentos_langchain.append(documento)
    
    # Text Splitter: Divide os documentos em peda√ßos menores (chunks)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=100
    )
    chunks = text_splitter.split_documents(documentos_langchain)
    
    logger.info(f'{len(chunks)} chunks criados a partir de {len(documentos_langchain)} documentos.')
    return chunks

```

3. Cria√ß√£o da Base de Conhecimento Vetorial (criar_base_vetorial)
    
    * Entrada: A lista de chunks.

    * Processo: Inicializa o modelo de embeddings e armazena os vetores dos chunks no ChromaDB.

    * Sa√≠da: Um objeto vectorstore do Chroma.

```python

# --- CRIA√á√ÉO DA BASE DE CONHECIMENTO VETORIAL ---
def criar_base_vetorial(chunks):
    '''
    Cria a base de dados de vetores usando ChromaDB e embeddings do Gemini AI.
    '''
    logger.info('Criando a base de dados vetorial (Vector Store)...')
    # Modelo de Embeddings: Transforma texto em vetores num√©ricos
    embeddings = GoogleGenerativeAIEmbeddings(model='models/text-embedding-004')

    # ChromaDB que roda em mem√≥ria.
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings
    )
    logger.info('Base de dados vetorial criada com sucesso!')
    return vectorstore

```

4. L√≥gica Principal de Orquestra√ß√£o
    
    * Define o caminho do CSV (file_author_csv).

    * Chama as fun√ß√µes de processamento e cria√ß√£o da base vetorial.

```python

# --- L√ìGICA PRINCIPAL DE ORQUESTRA√á√ÉO ---
# Definindo o caminho do arquivo CSV
file_author_csv = './documents/author_sem_duplicatas.csv'
chunks_processados_csv_author = processar_csv_author_preparar_chunks(file_author_csv)
vectorstore = criar_base_vetorial(chunks_processados_csv_author)

```

5. Constru√ß√£o do Pipeline de RAG
    * Retriever: Configura o mecanismo de busca (vectorstore.as_retriever).

    * Prompt Template: Define o comportamento do agente (template).

    * LLM: Inicializa o modelo ChatGoogleGenerativeAI.

    * Cadeia RAG: Monta o pipeline usando LCEL, conectando a busca de contexto (retriever), o prompt e a gera√ß√£o de resposta pelo LLM.

```python

# --- CONSTRU√á√ÉO DO PIPELINE DE RAG ---
logger.info('Configurando o pipeline de RAG...')

# O Retriever √© respons√°vel por buscar os chunks relevantes na base vetorial
retriever = vectorstore.as_retriever(search_kwargs={'k': 3}) # 'k' √© o n√∫mero de documentos a retornar

# O Prompt Template
template = '''
Voc√™ √© um especialista em informa√ß√µes sobre autores. 
Sua tarefa √© responder perguntas do usu√°rio usando apenas o contexto fornecido. 
Se a resposta n√£o estiver no contexto, diga que a informa√ß√£o n√£o est√° dispon√≠vel.

## Contexto

{contexto_recuperado}

## Pergunta

{pergunta_do_usuario}

## Instru√ß√µes para a Resposta
... (Instru√ß√µes detalhadas omitidas aqui para brevidade na documenta√ß√£o, mas presentes no c√≥digo)
'''

prompt = ChatPromptTemplate.from_template(template)

# O LLM (Large Language Model) que ir√° gerar a resposta final.
llm = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001', temperature=0.7)

# Constru√ß√£o da Cadeia (Chain) RAG com LangChain Expression Language (LCEL)
cadeia_rag = (
    {'contexto_recuperado': retriever, 'pergunta_do_usuario': RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

logger.info('Pipeline de RAG pronto!')
```