# Fun√ß√µes

<!--ts-->   
   * [Fun√ß√£o - raspagem_quotes_toscrape()](#scraper-de-cita√ß√µes--raspagem_quotes_toscrape)
   * [Fun√ß√£o - raspagem_page_author()](#raspador-de-autores--raspagem_page_author)
   * [Fun√ß√£o - cria_file_csv()](#cria_file_csv--exportador-csv)   
<!--te-->


# Scraper de Cita√ß√µes ‚Äî `raspagem_quotes_toscrape`

## üìå Descri√ß√£o

Este projeto implementa um scraper em Python que percorre todas as p√°ginas do site [Quotes to Scrape](https://quotes.toscrape.com) e coleta informa√ß√µes sobre cada cita√ß√£o: **autor, texto, tags** e a **URL da p√°gina**.  

O resultado √© retornado em formato estruturado (lista de dicion√°rios).

---

## ‚öôÔ∏è Funcionalidades
- Navega√ß√£o autom√°tica por todas as p√°ginas de cita√ß√µes.
- Extra√ß√£o de:
  - Autor da cita√ß√£o  
  - Texto da cita√ß√£o  
  - Lista de tags associadas  
  - URL da p√°gina de origem  
- Retorno dos dados em formato estruturado (list[dict]).

---

## üì¶ Depend√™ncias
Bibliotecas necess√°rias (instale com `pip`):

- [requests](https://pypi.org/project/requests/) ‚Üí para requisi√ß√µes HTTP  
- [beautifulsoup4](https://pypi.org/project/beautifulsoup4/) ‚Üí para parsing e navega√ß√£o no HTML  

Instala√ß√£o r√°pida:
```bash
pip install requests beautifulsoup4
```



## üóÇ Estrutura do C√≥digo

### Defini√ß√£o da fun√ß√£o

```python
def raspagem_quotes_toscrape(url: str) -> list[dict]:
```

* Cria uma fun√ß√£o chamada `raspagem_quotes_toscrape`.
* Recebe como entrada `url` (uma string) ‚Äî a p√°gina inicial do site de cita√ß√µes.
* Retorna uma lista de dicion√°rios (`list[dict]`), cada um representando uma cita√ß√£o.

---

### Inicializa√ß√£o de vari√°veis

```python
headers = {"User-Agent": "Mozilla/5.0 (compatible; ScraperBot/1.0)"}
page_url = "/"
resultados: list[dict] = []
```

* `headers`: define um "User-Agent" para a requisi√ß√£o HTTP, simulando um navegador.
* `page_url = "/"`: indica que come√ßamos pela p√°gina inicial do site.
* `resultados`: lista vazia que vai armazenar todas as cita√ß√µes encontradas.

---

### Mensagem de in√≠cio

```python
logger.info("Iniciando a raspagem de dados das cita√ß√µes...")
```

* Registra uma mensagem no log informando que o scraper come√ßou.
* `logger` √© usado para substituir `print` e ter controle de n√≠vel de mensagens (`info`, `error` etc.).

---

### Loop principal de p√°ginas

```python
while page_url:
    full_url = urljoin(url, page_url)
    logger.info("Raspando a p√°gina: %s", full_url)
```

* O loop `while` continua enquanto `page_url` tiver um valor (ou seja, enquanto houver pr√≥xima p√°gina).
* `urljoin(url, page_url)` combina a URL base com o caminho da p√°gina atual, formando a URL completa.
* Mostra no log a URL que est√° sendo raspada.

---

### Requisi√ß√£o HTTP

```python
try:
    resp = requests.get(full_url, headers=headers, timeout=10)
    resp.raise_for_status()
except requests.RequestException as e:
    logger.error("Falha ao acessar %s: %s", full_url, e)
    break
```

* Tenta acessar a p√°gina via `requests.get`.
* `timeout=10`: n√£o espera mais que 10 segundos por uma resposta.
* `resp.raise_for_status()`: lan√ßa um erro se o servidor responder com erro HTTP (ex.: 404, 500).
* Se houver erro (`RequestException`), registra no log e **encerra o loop** (`break`).

---

### Parse do HTML

```python
soup = BeautifulSoup(resp.text, "html.parser")
```

* Converte o HTML da p√°gina em um objeto `BeautifulSoup`, permitindo navegar e buscar elementos com facilidade.

---

### Extra√ß√£o das cita√ß√µes

```python
for q in soup.find_all("div", class_="quote"):
    tags = [a.get_text(strip=True) for a in q.find_all("a", class_="tag")]
    resultados.append({
        "autor":   q.find("small", class_="author").get_text(strip=True),
        "citacao": q.find("span", class_="text").get_text(strip=True),
        "tags":    tags,
        "pagina":  full_url
    })
```

* Para cada `<div class="quote">` na p√°gina:

  * Busca todas as tags associadas (`a.tag`) e transforma em uma lista de strings.
  * Extrai:

    * **Autor**: texto dentro de `<small class="author">`.
    * **Cita√ß√£o**: texto dentro de `<span class="text">`.
    * **Tags**: lista extra√≠da.
    * **P√°gina**: URL completa de onde a cita√ß√£o veio.
  * Adiciona todas essas informa√ß√µes como um **dicion√°rio** na lista `resultados`.

---

### Encontrar pr√≥xima p√°gina

```python
next_a = soup.select_one("li.next a")
page_url = next_a["href"] if next_a else None
```

* Procura o link da pr√≥xima p√°gina (`li.next a`).
* Se existir, atualiza `page_url` com o `href`; caso contr√°rio, define como `None` para encerrar o loop.

---

### Retorno

```python
return resultados
```

* Retorna a lista de dicion√°rios com as cita√ß√µes completas.

---


## ‚ñ∂Ô∏è Como Executar

1. Clone o reposit√≥rio ou copie o arquivo Python.
2. Crie um ambiente virtual (opcional, recomendado):

   ```bash
   python -m venv .venv
   source .venv/bin/activate   # Linux / Mac
   .venv\Scripts\activate      # Windows
   ```
3. Instale as depend√™ncias:

   ```bash
   pip install -r requirements.txt
   ```

   ou diretamente:

   ```bash
   pip install requests beautifulsoup4
   ```

---

## üìù Exemplo de Uso

Shell Python:

```python
>>> from scraper import raspagem_quotes_toscrape
>>> import json


>>> url = "https://quotes.toscrape.com"
>>> resultados = raspagem_quotes_toscrape(url)
>>> print(json.dumps(resultados, ensure_ascii=False, indent=2))
```

---

## üìä Sa√≠da Esperada

O retorno ser√° uma lista de dicion√°rios, por exemplo:

```json
[
  {
    "autor": "Albert Einstein",
    "citacao": "‚ÄúThe world as we have created it is a process of our thinking...‚Äù",
    "tags": ["change","deep-thoughts","thinking","world"],
    "pagina": "https://quotes.toscrape.com/page/1/"
  },
  ...
  {
    "autor": "Jane Austen",
    "citacao": "‚ÄúIt is a truth universally acknowledged...‚Äù",
    "tags": ["truth","love"],
    "pagina": "https://quotes.toscrape.com/page/2/"
  }
]
```

---

## üí° C√≥digo Principal

```python

def raspagem_quotes_toscrape(url: str) -> list[dict]:
    headers = {"User-Agent": "Mozilla/5.0 (compatible; ScraperBot/1.0)"}
    page_url = "/"
    resultados: list[dict] = []

    logging.info("Iniciando a raspagem de dados das cita√ß√µes...")

    while page_url:
        full_url = urljoin(url, page_url)
        logging.info("Raspando a p√°gina: %s", full_url)

        try:
            resp = requests.get(full_url, headers=headers, timeout=10)
            resp.raise_for_status()
        except requests.RequestException as e:
            logging.error("Falha ao acessar %s: %s", full_url, e)
            break

        soup = BeautifulSoup(resp.text, "html.parser")

        for q in soup.find_all("div", class_="quote"):
            tags = [a.get_text(strip=True) for a in q.find_all("a", class_="tag")]
            resultados.append({
                "autor":   q.find("small", class_="author").get_text(strip=True),
                "citacao": q.find("span", class_="text").get_text(strip=True),
                "tags":    tags,
                "pagina":  full_url
            })

        next_a = soup.select_one("li.next a")
        page_url = next_a["href"] if next_a else None

    return resultados
```


# Raspador de Autores ‚Äî `raspagem_page_author`

## üìå Descri√ß√£o

Fun√ß√£o √∫nica que percorre p√°ginas paginadas de um site (p. ex. `quotes.toscrape.com`), encontra links "(about)" relativos a autores, abre cada p√°gina de autor e extrai informa√ß√µes b√°sicas (nome, data de nascimento, local e descri√ß√£o). Retorna uma lista de dicion√°rios com os dados coletados.

---

## ‚öôÔ∏è Funcionalidades

- Percorre p√°ginas paginadas do site seguindo o link `li.next a`.
- Encontra links de autor cujo texto do anchor √© exatamente `"(about)"`.
- Acessa cada p√°gina de autor (deduplicadas) e extrai:
  - Nome do autor (`h3.author-title`)
  - Data de nascimento (`span.author-born-date`)
  - Local de nascimento (`span.author-born-location`)
  - Descri√ß√£o (`div.author-description`) ‚Äî com limpeza simples de v√≠rgulas, pontos e aspas.
- Retorna todos os registros em uma lista Python (`List[Dict[str, str]]`).

---

## üì¶ Depend√™ncias

Bibliotecas necess√°rias (pelo menos):

- `requests` ‚Äî requisi√ß√µes HTTP.
- `beautifulsoup4` (`bs4`) ‚Äî parsing e extra√ß√£o de HTML.
- `re` ‚Äî express√µes regulares (m√≥dulo padr√£o do Python).
- `urllib` (m√≥dulo padr√£o) ‚Äî usado no c√≥digo original com `urlopen`.

---

## üóÇ Estrutura do C√≥digo


### Defini√ß√£o da fun√ß√£o

```python
def raspagem_page_author(url: str, delay: float = 0.5, timeout: int = 10) -> List[Dict[str, str]]:
```

* `url`: endere√ßo base do site para come√ßar a raspagem.
* `delay`: tempo em segundos entre requisi√ß√µes (para n√£o sobrecarregar o site).
* `timeout`: tempo m√°ximo de espera para respostas HTTP.
* Retorno: uma lista de dicion√°rios (`List[Dict[str,str]]`) com informa√ß√µes dos autores.

---

### Inicializa√ß√£o

```python
session = requests.Session()
session.headers.update({"User-Agent": "Mozilla/5.0 (compatible; my-scraper/1.0)"})
author_data: List[Dict[str, str]] = []
visited_author_urls = set()
next_path: Optional[str] = "/"
```

* Cria uma **sess√£o** HTTP para reutilizar conex√µes.
* Define um **User-Agent** para se identificar ao servidor.
* `author_data`: lista que vai guardar todos os autores coletados.
* `visited_author_urls`: conjunto para evitar visitar a mesma p√°gina de autor mais de uma vez.
* `next_path`: guarda o caminho da pr√≥xima p√°gina (inicia com `/`).

---

### Loop principal das p√°ginas

```python
while next_path:
    page_url = urljoin(url, next_path)
```

* Enquanto existir uma pr√≥xima p√°gina (`next_path`), a fun√ß√£o continua.
* `urljoin` cria a URL completa da p√°gina atual.

---

###  Requisi√ß√£o e parsing da p√°gina

```python
resp = session.get(page_url, timeout=timeout)
soup = BeautifulSoup(resp.text, "html.parser")
```

* Faz o **GET HTTP** para baixar a p√°gina.
* Usa o **BeautifulSoup** para transformar o HTML em objeto naveg√°vel.

---

###  Buscar links de autores

```python
about_anchors = soup.find_all("a", string=lambda s: s and "(about)" in s)
```

* Procura todos os `<a>` cujo texto cont√©m `(about)`.
* Cada link leva √† p√°gina de detalhes do autor.

---

### Loop pelos autores da p√°gina

```python
for ah in about_anchors:
    href = ah.get("href")
    if not href: continue
    author_url = urljoin(url, href)
    if author_url in visited_author_urls: continue
    visited_author_urls.add(author_url)
```

* Constr√≥i a URL completa de cada autor.
* Pula se o link j√° foi visitado.
* Marca como visitado adicionando ao `set`.

---

###  Requisi√ß√£o e parsing da p√°gina do autor

```python
r2 = session.get(author_url, timeout=timeout)
soup_a = BeautifulSoup(r2.text, "html.parser")
```

* Faz GET na p√°gina do autor.
* Converte HTML em `BeautifulSoup` para extrair dados.

---

### Extra√ß√£o de dados do autor

```python
name_tag = soup_a.find("h3", class_="author-title")
date_tag = soup_a.find("span", class_="author-born-date")
place_tag = soup_a.find("span", class_="author-born-location")
desc_tag = soup_a.find("div", class_="author-description")

name = name_tag.get_text(strip=True) if name_tag else ""
born_date = date_tag.get_text(strip=True) if date_tag else ""
born_location = place_tag.get_text(strip=True) if place_tag else ""
descricao = desc_tag.get_text(" ", strip=True) if desc_tag else ""
descricao = re.sub(r'["\u201c\u201d]', '', descricao)
```

* Extrai **nome, data, local e descri√ß√£o**.
* Usa `get_text(strip=True)` para limpar espa√ßos.
* Remove aspas e caracteres especiais da descri√ß√£o com regex.

---

### Armazenar dados do autor

```python
author_data.append({
    "author": name,
    "data_nascimento": born_date,
    "local_nascimento": born_location,
    "descricao": descricao
})
```

* Adiciona um **dicion√°rio** com os dados √† lista `author_data`.

---

### Aguardar entre requisi√ß√µes

```python
time.sleep(delay)
```

* Espera o tempo definido em `delay` para n√£o sobrecarregar o site.

---

### Encontrar pr√≥xima p√°gina

```python
next_li = soup.select_one("li.next a")
if next_li and next_li.get("href"):
    next_path = next_li["href"]
else:
    next_path = None
```

* Procura o link de **pr√≥xima p√°gina**.
* Se n√£o houver, encerra o loop.

---

### Retorno final

```python
return author_data
```

* Retorna a lista de Dicion√°rios completa de autores coletados.

---

Se voc√™ quiser, posso desenhar um **diagrama simples** mostrando o fluxo do c√≥digo ‚Äî isso ajuda muito a visualizar como as p√°ginas e autores s√£o percorridos.

Quer que eu fa√ßa?


---

## ‚ñ∂Ô∏è Como Executar

1. Clone o reposit√≥rio ou copie o arquivo Python.
2. Crie um ambiente virtual (opcional, recomendado):

   ```bash
   python -m venv .venv
   source .venv/bin/activate   # Linux / Mac
   .venv\Scripts\activate      # Windows
   ```
3. Instale as depend√™ncias:

   ```bash
   pip install -r requirements.txt
   ```

   ou diretamente:

   ```bash
   pip install requests beautifulsoup4
   ```

---

## üìù Exemplo de Uso

Shell Python:

```python
>>> from scraper import raspagem_quotes_toscrape
>>> import json


>>> url = "https://quotes.toscrape.com"
>>> resultados = raspagem_page_author(url)
>>> print(json.dumps(resultados, ensure_ascii=False, indent=2))
```
---

## üìä Sa√≠da Esperada

A fun√ß√£o retorna `List[Dict[str, str]]`. Cada item √© um dicion√°rio com as chaves (nomes como no c√≥digo original):

- `author` ‚Äî nome do autor (string)
- `data nascimento` ‚Äî data de nascimento (string)
- `local nascimento` ‚Äî local de nascimento (string)
- `descricao` ‚Äî descri√ß√£o do autor (string, com algumas pontua√ß√µes removidas)

Exemplo de item:

```py
{
  'author': 'Albert Einstein',
  'data nascimento': 'March 14, 1879',
  'local nascimento': 'in Ulm, Germany',
  'descricao': 'F√≠sico te√≥rico conhecido por...'
}
```
---

## üí° C√≥digo Principal

```Python
def raspagem_page_author(url: str, delay: float = 0.5, timeout: int = 10) -> List[Dict[str, str]]:
    """
    Raspagem de autores a partir de um site paginado.
    Retorna lista de dicion√°rios com chaves: 'author', 'data_nascimento', 'local_nascimento', 'descricao'.
    """
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0 (compatible; my-scraper/1.0)"})
    author_data: List[Dict[str, str]] = []
    visited_author_urls = set()
    
    next_path: Optional[str] = "/"

    while next_path:
        page_url = urljoin(url, next_path)
        logger.info(f"Raspando p√°gina: {page_url}")
        try:
            resp = session.get(page_url, timeout=timeout)
            resp.raise_for_status()
        except requests.RequestException as e:
            logger.exception(f"Falha ao acessar {page_url}: {e}")
            break

        soup = BeautifulSoup(resp.text, "html.parser")
        
        about_anchors = soup.find_all("a", string=lambda s: s and "(about)" in s)

        for ah in about_anchors:
            href = ah.get("href")
            if not href:
                continue
            author_url = urljoin(url, href)
            if author_url in visited_author_urls:
                continue
            visited_author_urls.add(author_url)

            logger.info(f"Raspando autor: {author_url}")
            try:
                r2 = session.get(author_url, timeout=timeout)
                r2.raise_for_status()
            except requests.RequestException as e:
                logger.warning(f"Erro ao acessar p√°gina do autor {author_url}: {e}")
                continue

            soup_a = BeautifulSoup(r2.text, "html.parser")
            # uso de get_text(strip=True) e checagem de None
            name_tag = soup_a.find("h3", class_="author-title")
            date_tag = soup_a.find("span", class_="author-born-date")
            place_tag = soup_a.find("span", class_="author-born-location")
            desc_tag = soup_a.find("div", class_="author-description")

            name = name_tag.get_text(strip=True) if name_tag else ""
            born_date = date_tag.get_text(strip=True) if date_tag else ""
            born_location = place_tag.get_text(strip=True) if place_tag else ""
            descricao = desc_tag.get_text(" ", strip=True) if desc_tag else ""
            # limpeza controlada (remove quebras de linha e aspas extras)
            descricao = re.sub(r'["\u201c\u201d]', '', descricao)

            author_data.append({
                "author": name,
                "data_nascimento": born_date,
                "local_nascimento": born_location,
                "descricao": descricao
            })

            time.sleep(delay)  # respeitar servidor

        # Pegar link "next"
        next_li = soup.select_one("li.next a")
        if next_li and next_li.get("href"):
            next_path = next_li["href"]
        else:
            next_path = None

        time.sleep(delay)

    return author_data
```

---



# cria_file_csv ‚Äî Exportador CSV

## üìå Descri√ß√£o

Fun√ß√£o utilit√°ria para exportar uma cole√ß√£o de registros (cada registro representado por um dicion√°rio Python) para um arquivo CSV. A fun√ß√£o determina os nomes de coluna (fieldnames) a partir das chaves dos dicion√°rios e grava o cabe√ßalho seguido das linhas de dados.

---

## ‚öôÔ∏è Funcionalidades

- Grava uma lista/iter√°vel de dicion√°rios em um arquivo CSV.
- Permite usar a uni√£o de todas as chaves encontradas nos registros como cabe√ßalho (ou apenas as chaves do primeiro registro).
- Trata cria√ß√£o de diret√≥rios ausentes automaticamente.
- Abre o arquivo com `newline=''` e codifica√ß√£o configur√°vel (recomendado `utf-8-sig` para compatibilidade com Excel no Windows).
- Permite controlar o comportamento para chaves extras (`extrasaction='ignore'` ou `'raise'`).
- Registra erros de I/O com `logging` e re-levanta exce√ß√µes para tratamento externo.

---


## üì¶ Depend√™ncias

- M√≥dulo padr√£o `csv` ‚Äî escrita em CSV usando `DictWriter`.
- `pathlib.Path` ‚Äî manipula√ß√£o de caminhos e cria√ß√£o de diret√≥rios.
- `logging` ‚Äî para notifica√ß√µes de erro (opcional, parte da implementa√ß√£o sugerida).
- (Opcional) `pandas` ‚Äî alternativa conveniente (`DataFrame.to_csv`) para conjuntos de dados maiores ou transforma√ß√µes adicionais.

> Observa√ß√£o: a vers√£o m√≠nima requer apenas o m√≥dulo `csv` da biblioteca padr√£o. `pathlib` e `logging` tamb√©m fazem parte da biblioteca padr√£o do Python.

---

